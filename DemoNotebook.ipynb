{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c9fdf6-6276-45dd-bf4b-c0b7c022b423",
   "metadata": {},
   "source": [
    "# Demo notebook for *Identification of Dialect for Eastern and Southwestern Ojibwe Words Using a Small Corpus*\n",
    "### AmericasNLP 2023 submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8332ad-d4fb-46a7-a12f-cd058f3b3bed",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdecbf69-81bf-43e6-aee7-f1c475ecd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import operator\n",
    "import copy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import  linear_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8bc82-fd8e-4dfb-b2c2-9cabb65d66e0",
   "metadata": {},
   "source": [
    "Helper function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a08830-2cae-44d2-b500-c8b13f2365ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading code for different data formats \n",
    "\n",
    "#onj is journal - lots of stories in one text file\n",
    "def readonj(file):\n",
    "    with open(file) as inputFile:\n",
    "        text = inputFile.read()\n",
    "        sentences = text.replace(',','').replace('(','').replace('-',' ').replace(')','').replace('\"','').replace(';',' ').replace(':','').replace('“','').replace('!','').replace('?','').replace('\\n','').replace('”','').replace(':','').lower().split('.')\n",
    "    return sentences[:-1] #remove final split dangler\n",
    "\n",
    "#eastern dialect stories are one story per file\n",
    "def reade(file):\n",
    "    with open(file) as inputFile:\n",
    "        text = inputFile.read()\n",
    "        text = re.sub(r'\\d', '', text) #remove digits\n",
    "        sentences = text.replace('(','').replace(';',' ').replace('-',' ').replace(')','').replace(',','').replace(':','').replace('“','').replace('!','').replace('?','').replace('\\n','').replace('”','').replace(':','').lower().split('.')\n",
    "    return sentences[:-1] #remove final split dangler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ngram generators\n",
    "def word2ngrams(text, n=3):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "def sent2ngrams_counted(text, n=3):\n",
    "    nGrams = list(chain(*[word2ngrams(i,n) for i in text.replace('.','').lower().split()]))\n",
    "    return nGrams\n",
    "    \n",
    "def TF_nGrams(nGrams):\n",
    "    wordSet = set(nGrams)\n",
    "    wordDict = dict.fromkeys(wordSet, 0) \n",
    "    for word in nGrams:\n",
    "        wordDict[word]+=1/len(nGrams)\n",
    "    return wordDict\n",
    "\n",
    "def sorted_nGram_dict(wordDict):\n",
    "    sorted_dict = sorted(wordDict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "def compute_n_grams(sentences,n,start_n=2):\n",
    "    all_ngrams = {}\n",
    "    for nn in np.arange(start_n,n+1):\n",
    "        ngrams = []\n",
    "        if type(sentences) is str:\n",
    "            ngrams.extend(sent2ngrams_counted(sentences,n=nn))\n",
    "        else:\n",
    "            for sentence in sentences:\n",
    "                ngrams.extend(sent2ngrams_counted(sentence,n=nn))\n",
    "            #normalize per ngram size\n",
    "        all_ngrams = {**all_ngrams, **TF_nGrams(ngrams)}\n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad63171-69b5-4641-bd0d-0032a2e5f577",
   "metadata": {},
   "source": [
    "Because text files are not shared with this demo notebook, demonstration of model on word list is the only live part of the shared code. The rest of the notebook is shared as documentation to assist others in the future who want to replicate this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b93724-4a76-4529-af50-6aadd860ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained model\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9a0c67-958f-430e-96c2-b7ff1ac4b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load initialized empty dictionary to have all n-grams found\n",
    "with open('saved_nGram_dictionary.pkl', 'rb') as f:\n",
    "    initialized_nGram_dict_sw_e = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb68897f-5bd6-4547-a60e-853ba7d1d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load word list and create labels - use 1 for SW and 0 for E\n",
    "df = pd.read_csv('./data/fiftyOjibweWordsE_SW.csv')\n",
    "words = df['Ojibwe SW'].to_list() + df['Ojibwe E/Odawa'].to_list()\n",
    "labels = list(np.ones((50))) + list(np.zeros((50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef81a74d-6a06-47b0-8385-f9082d397a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.72      0.71        50\n",
      "         1.0       0.71      0.70      0.71        50\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.71      0.71      0.71       100\n",
      "weighted avg       0.71      0.71      0.71       100\n",
      "\n",
      "[[36 14]\n",
      " [15 35]]\n"
     ]
    }
   ],
   "source": [
    "#Create features for words and load into dictionary with all n-grams\n",
    "ngrams_count = 4\n",
    "featurized_words =  np.zeros((len(initialized_nGram_dict_sw_e),len(words)))\n",
    "for index,sentence in enumerate(featurized_words.T):\n",
    "    features = compute_n_grams(words[index],ngrams_count)\n",
    "    new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "    for something in features.keys():\n",
    "            if something in new_dict.keys():\n",
    "                new_dict[something] = features[something]\n",
    "    featurized_words[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "\n",
    "#Make predictions on words    \n",
    "word_results = clf.predict(featurized_words.T)\n",
    "\n",
    "#Run reports on results\n",
    "print(classification_report(labels,word_results))\n",
    "print(confusion_matrix(labels,word_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a154377-fea5-4d28-9ec3-915488d74a28",
   "metadata": {},
   "source": [
    "For those reading the paper, please note that we have manually removed the 3 repeated words from the SW Ojibwe counts in the confusion matrix and calculation of accuracy in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd2d40-f6ca-4411-b955-1c456eec66e3",
   "metadata": {},
   "source": [
    "## Everything after this point is meant for documentation and replication only and is not meant to be a live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d983aa-7efb-4671-93e0-525a9ae8b4d8",
   "metadata": {},
   "source": [
    "Data loading code, commented because we don't have permission to reproduce full texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db393f45-fc36-4e03-8ef6-621db302e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SW data is all combined into one text file\n",
    "# file = './data/sw/onj8_1.txt'\n",
    "# southwestern_sentences = readonj(file)\n",
    "# print(len(southwestern_sentences) + 'total sentences')\n",
    "# print(np.mean([len(a.split(' ')) for a in southwestern_sentences]) + 'average words per sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40612dce-801e-412e-afa2-3d3e1ed8bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # E data is separate files per story\n",
    "# directory = './data/e'\n",
    "# files = os.listdir(directory)\n",
    "# eastern_sentences = []\n",
    "# for file in files:\n",
    "#     if os.path.isfile(directory + '/' + file):\n",
    "#         eastern_sentences.extend(reade(directory + '/' +file))\n",
    "# print(len(eastern_sentences) + 'total sentences')\n",
    "# print(np.mean([len(a.split(' ')) for a in eastern_sentences]) + 'average words per sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c614d2-a09b-4e1c-becb-2dd925c5e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all data to capture all n-grams so we have everything to start.\n",
    "# # Commented because stories are not shared\n",
    " \n",
    "# all_sw_ngrams = compute_n_grams(southwestern_sentences,ngrams_count,start_n=1)\n",
    "# all_e_ngrams = compute_n_grams(eastern_sentences,ngrams_count,start_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fc0d6-2c55-4fa2-9613-24b9e26fd2f8",
   "metadata": {},
   "source": [
    "Combine all the ngrams possible and create a mega list so that we can properly initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179416d0-bfe8-4d50-b97b-998b4290e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined_keys = list(TF_nGrams(list(all_sw_ngrams.keys()) + list(all_e_ngrams.keys())).keys())\n",
    "# len(combined_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad39950-f962-4d2e-909c-8d4a7bcf4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized_nGram_dict = {key: 0 for key in combined_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b4a571-ecc5-450d-8e2e-56f2122e271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_keys = list(TF_nGrams(list(all_sw_ngrams.keys()) + list(all_e_ngrams.keys())))\n",
    "# print(len(combined_keys))\n",
    "# initialized_nGram_dict_sw_e = {key: 0 for key in combined_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c379389-7a5b-41c1-93c7-a147598975f0",
   "metadata": {},
   "source": [
    "Create combined and labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba1f81c-edb3-4c82-97bd-ed0ee6c7fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = eastern_sentences + southwestern_sentences\n",
    "# Y = np.concatenate((np.zeros(len(eastern_sentences)),np.ones(len(southwestern_sentences))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dcb62-d22d-417c-92c6-f877708f766a",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4f3322-1df3-4e16-9db5-195f78973089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber was initially tested, didn't work well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da59345-ab30-4c11-95f5-bff62ef5fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #5 fold cross validation\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "# #save weights and average for final model\n",
    "# all_weights = np.zeros((len(initialized_nGram_dict_sw_e),))\n",
    "\n",
    "\n",
    "# #initialize feature space    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e),len(X)))\n",
    "\n",
    "# #build features\n",
    "# for index,sentence in enumerate(featurized_x.T):\n",
    "#     features = compute_n_grams(X[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# #run the cross validation\n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "#     clf.fit(featurized_x[:,train_index].T, Y[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y[test_index])\n",
    "#     all_weights += np.asarray(clf[1].coef_[0])\n",
    "\n",
    "# #compute and display combined results\n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n",
    "\n",
    "\n",
    "# #show most influential n-grams\n",
    "# weight_dict = dict(zip(list(initialized_nGram_dict_sw_e.keys()),all_weights/5))\n",
    "# print(sorted_nGram_dict(weight_dict)[:10])\n",
    "# print(sorted_nGram_dict(weight_dict)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb99a0a-0c54-4e60-9bfb-c3966f85badd",
   "metadata": {},
   "source": [
    "Repeat for 2 sentence groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d36d45e-3e35-45d2-8ccb-13e94bd43977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# # skf.get_n_splits(X, Y)\n",
    "# all_weights = np.zeros((len(initialized_nGram_dict_sw_e),))\n",
    "\n",
    "\n",
    "# X2 = [ x+' '+y for x,y in zip(eastern_sentences[0::2], eastern_sentences[1::2]) ] + [ x+y for x,y in zip(southwestern_sentences[0::2], southwestern_sentences[1::2]) ]\n",
    "# Y2 = np.concatenate((np.zeros(len(eastern_sentences[0::2])),np.ones(len(southwestern_sentences[0::2]))))\n",
    "\n",
    "\n",
    "\n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e),len(X2)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x.T):\n",
    "#     features = compute_n_grams(X2[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X2, Y2)):\n",
    "#     clf.fit(featurized_x[:,train_index].T, Y2[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y2[test_index])\n",
    "#     all_weights += np.asarray(clf[1].coef_[0])\n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n",
    "\n",
    "\n",
    "\n",
    "# weight_dict = dict(zip(list(initialized_nGram_dict_sw_e.keys()),all_weights/5))\n",
    "# print(sorted_nGram_dict(weight_dict)[:10])\n",
    "# print(sorted_nGram_dict(weight_dict)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9a9db-c857-4ab1-94fc-3bfb01d32f8e",
   "metadata": {},
   "source": [
    "Repeat for 3 sentence groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d17729-3cf2-46f0-82d2-cef08a0d3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# # skf.get_n_splits(X, Y)\n",
    "# all_weights = np.zeros((len(initialized_nGram_dict_sw_e),))\n",
    "\n",
    "\n",
    "# p1 = [ x+' '+y +' ' + z for x,y,z in zip(eastern_sentences[0::3], eastern_sentences[1::3], eastern_sentences[2::3]) ]\n",
    "# p2 = [  x+' '+y +' ' + z for x,y,z in zip(southwestern_sentences[0::3], southwestern_sentences[1::3], southwestern_sentences[2::3]) ]\n",
    "# X3 = p1 + p2\n",
    "\n",
    "# Y3 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))\n",
    "\n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e),len(X3)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x.T):\n",
    "#     features = compute_n_grams(X3[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X3, Y3)):\n",
    "#     clf.fit(featurized_x[:,train_index].T, Y3[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y3[test_index])\n",
    "#     all_weights += np.asarray(clf[1].coef_[0])\n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n",
    "\n",
    "\n",
    "\n",
    "# weight_dict = dict(zip(list(initialized_nGram_dict_sw_e.keys()),all_weights/5))\n",
    "# print(sorted_nGram_dict(weight_dict)[:10])\n",
    "# print(sorted_nGram_dict(weight_dict)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16aab7-d45b-4263-a155-ac9c3c615a1f",
   "metadata": {},
   "source": [
    "Repeat for 4 sentence groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c10995c0-8008-4c96-9cf3-77f98bf41e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# # skf.get_n_splits(X, Y)\n",
    "# all_weights = np.zeros((len(initialized_nGram_dict_sw_e),))\n",
    "\n",
    "# p1 = [ x+' '+y +' ' + z+' ' + a for x,y,z,a in zip(eastern_sentences[0::4], eastern_sentences[1::4], eastern_sentences[2::4], eastern_sentences[3::4]) ]\n",
    "# p2 = [ x+' '+y +' ' + z+' ' + a for x,y,z,a in zip(southwestern_sentences[0::4], southwestern_sentences[1::4], southwestern_sentences[2::4], southwestern_sentences[3::4]) ]\n",
    "# X4 = p1 + p2\n",
    "\n",
    "# Y4 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))\n",
    "\n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e),len(X4)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x.T):\n",
    "#     features = compute_n_grams(X4[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X4, Y4)):\n",
    "#     clf.fit(featurized_x[:,train_index].T, Y4[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y4[test_index])\n",
    "#     all_weights += np.asarray(clf[1].coef_[0])\n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n",
    "\n",
    "\n",
    "\n",
    "# weight_dict = dict(zip(list(initialized_nGram_dict_sw_e.keys()),all_weights/5))\n",
    "# print(sorted_nGram_dict(weight_dict)[:10])\n",
    "# print(sorted_nGram_dict(weight_dict)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7ec2e-8ec5-4ee1-a516-bc56b40ac907",
   "metadata": {},
   "source": [
    "Repeat for 5 sentence groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e80122b-bb8b-4beb-b896-1e6de1e1c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# # skf.get_n_splits(X, Y)\n",
    "# all_weights = np.zeros((len(initialized_nGram_dict_sw_e),))\n",
    "\n",
    "# p1 = [ x+' '+y +' ' + z+' ' + a+' ' + b  for x,y,z,a,b in zip(eastern_sentences[0::5], eastern_sentences[1::5], eastern_sentences[2::5], eastern_sentences[3::5], eastern_sentences[4::5]) ]\n",
    "# p2 = [ x+' '+y +' ' + z+' ' + a+' ' + b for x,y,z,a,b in zip(southwestern_sentences[0::5], southwestern_sentences[1::5], southwestern_sentences[2::5], southwestern_sentences[3::5], southwestern_sentences[4::5]) ]\n",
    "# X5 = p1 + p2\n",
    "\n",
    "# Y5 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))\n",
    "\n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e),len(X5)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x.T):\n",
    "#     features = compute_n_grams(X5[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X5, Y5)):\n",
    "#     clf.fit(featurized_x[:,train_index].T, Y5[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y5[test_index])\n",
    "#     all_weights += np.asarray(clf[1].coef_[0])\n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n",
    "\n",
    "\n",
    "\n",
    "# weight_dict = dict(zip(list(initialized_nGram_dict_sw_e.keys()),all_weights/5))\n",
    "# print(sorted_nGram_dict(weight_dict)[:10])\n",
    "# print(sorted_nGram_dict(weight_dict)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90e3a5-5521-4b9f-b368-f99ffc753f38",
   "metadata": {},
   "source": [
    "Original word test based on 5 sentence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04e35aac-d02f-4525-9362-770868b01325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/fiftyOjibweWordsE_SW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ed4ae06-e736-4ad7-8dc4-1d3f9f2cbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = df['Ojibwe SW'].to_list() + df['Ojibwe E/Odawa'].to_list()\n",
    "# labels = list(np.ones((50))) + list(np.zeros((50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe73626-42ed-4144-92c7-5b63adfa932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurized_words =  np.zeros((len(initialized_nGram_dict_sw_e),len(words)))\n",
    "# for index,sentence in enumerate(featurized_words.T):\n",
    "#     features = compute_n_grams(words[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_words[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "\n",
    "    \n",
    "# word_results = clf.predict(featurized_words.T)\n",
    "# print(classification_report(labels,word_results))\n",
    "# print(confusion_matrix(labels,word_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b0dea7-f68d-4720-ae8b-86fefb47743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sloppy export of results for copy/paste into results spreadsheet\n",
    "# for i in np.arange(100):\n",
    "# #     print(words[i],word_results[i],words[i+50],word_results[i+50])\n",
    "#     if word_results[i]==1:\n",
    "#         print('SW')\n",
    "#     else:\n",
    "#         print('E')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44925780-26c8-4389-8eeb-c6ebed28ed9b",
   "metadata": {},
   "source": [
    "Build truncated n_gram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0205b51c-1d7b-4a97-b5f4-b0604a8b5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top100_e_ngrams = dict(sorted_nGram_dict(all_e_ngrams)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bfbaf1c-1e2b-441e-a240-47ec61d3d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top100_sw_ngrams = dict(sorted_nGram_dict(all_sw_ngrams)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0ba9182-6c78-44ab-9672-376c5473e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_key_overlap(dict1,dict2):\n",
    "    overlap_count = 0\n",
    "    overlap_keys = []\n",
    "    all_keys = []\n",
    "    for ngram in dict2.keys():\n",
    "        all_keys.append(ngram)\n",
    "        if ngram in dict1.keys():\n",
    "            overlap_count +=1\n",
    "            overlap_keys.append(ngram)\n",
    "    for ngram in dict1.keys():\n",
    "        all_keys.append(ngram)\n",
    "    all_keys = list(set(all_keys))\n",
    "    return overlap_count, overlap_keys, all_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a0b4c-7566-4980-9a7c-746004bc1f69",
   "metadata": {},
   "source": [
    "Compute overlap size for truncated dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "570351f0-ee3a-4b39-bcc3-9b05b5951b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for size in [100,500,1000]:\n",
    "#     overlap, shared_keys, all_keys = dict_key_overlap(dict(sorted_nGram_dict(all_e_ngrams)[:size]),dict(sorted_nGram_dict(all_sw_ngrams)[:size]))\n",
    "#     print(size)\n",
    "#     print(overlap)\n",
    "#     print(len(all_keys))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57388e6-18d3-491a-b34c-15865dd7e349",
   "metadata": {},
   "source": [
    "Initialize small truncated dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0121a6-22d4-42a9-be71-7e66aa034ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap, shared_keys, all_keys = dict_key_overlap(top100_e_ngrams,top100_sw_ngrams)\n",
    "\n",
    "# initialized_nGram_dict_sw_e_100 = {key: 0 for key in all_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08aaed5f-c0a4-4755-bf10-157588846c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(initialized_nGram_dict_sw_e_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c78771-fd14-46ff-96ce-043c4a1e03d1",
   "metadata": {},
   "source": [
    "Single sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e9bd6bf-9594-41be-bf3e-5d1c0acad7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "\n",
    "# clf2 = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber\n",
    "\n",
    "\n",
    "    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e_100),len(X)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x):\n",
    "#     features = compute_n_grams(X[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e_100)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "#     clf2.fit(featurized_x[:,train_index].T, Y[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf2.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y[test_index])\n",
    "    \n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fb787-b551-404d-bd9f-7c4dbf6c795b",
   "metadata": {},
   "source": [
    "Two sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c46d3580-baf5-4fa5-82e4-a37984cd51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = [ x+' '+y for x,y in zip(eastern_sentences[0::2], eastern_sentences[1::2]) ] + [ x+y for x,y in zip(southwestern_sentences[0::2], southwestern_sentences[1::2]) ]\n",
    "# Y2 = np.concatenate((np.zeros(len(eastern_sentences[0::2])),np.ones(len(southwestern_sentences[0::2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeee0270-1a1e-4ceb-9beb-ccf7da4af1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# skf.get_n_splits(X2, Y2)\n",
    "\n",
    "# clf2 = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber\n",
    "\n",
    "\n",
    "    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e_100),len(X2)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x):\n",
    "#     features = compute_n_grams(X2[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e_100)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X2, Y2)):\n",
    "#     clf2.fit(featurized_x[:,train_index].T, Y2[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf2.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y2[test_index])\n",
    "    \n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d82fd-e18f-41e7-867f-a8a6e25f34e0",
   "metadata": {},
   "source": [
    "Three sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c76dd16b-f41e-4779-822a-e144ebbfdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = [ x+' '+y +' ' + z for x,y,z in zip(eastern_sentences[0::3], eastern_sentences[1::3], eastern_sentences[2::3]) ]\n",
    "# p2 = [  x+' '+y +' ' + z for x,y,z in zip(southwestern_sentences[0::3], southwestern_sentences[1::3], southwestern_sentences[2::3]) ]\n",
    "# X3 = p1 + p2\n",
    "\n",
    "# Y3 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b19feb3d-d90d-4200-9aae-0bc4371b5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# skf.get_n_splits(X3, Y3)\n",
    "\n",
    "# clf2 = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber\n",
    "\n",
    "\n",
    "    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e_100),len(X3)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x):\n",
    "#     features = compute_n_grams(X3[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e_100)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X3, Y3)):\n",
    "#     clf2.fit(featurized_x[:,train_index].T, Y3[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf2.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y3[test_index])\n",
    "    \n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae74a3-8b5d-4c63-872b-4e6d7e0045b6",
   "metadata": {},
   "source": [
    "Four sentence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11b62f4f-b557-4963-8567-1bd09bdff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = [ x+' '+y +' ' + z+' ' + a for x,y,z,a in zip(eastern_sentences[0::4], eastern_sentences[1::4], eastern_sentences[2::4], eastern_sentences[3::4]) ]\n",
    "# p2 = [ x+' '+y +' ' + z+' ' + a for x,y,z,a in zip(southwestern_sentences[0::4], southwestern_sentences[1::4], southwestern_sentences[2::4], southwestern_sentences[3::4]) ]\n",
    "# X4 = p1 + p2\n",
    "\n",
    "# Y4 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "273ec835-6448-49f9-8eae-9c3bbb12f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# skf.get_n_splits(X4, Y4)\n",
    "\n",
    "# clf2 = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber\n",
    "\n",
    "\n",
    "    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e_100),len(X4)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x):\n",
    "#     features = compute_n_grams(X4[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e_100)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X4, Y4)):\n",
    "#     clf2.fit(featurized_x[:,train_index].T, Y4[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf2.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y4[test_index])\n",
    "    \n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results))\n",
    "# print(confusion_matrix(truth_labels,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962c649-df1e-4d54-b736-a87bc0109a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96746e94-dbb3-444c-a6ba-5372a73ec944",
   "metadata": {},
   "source": [
    "Five sentence test with truncated feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f304ec2-e35c-4c1d-9004-3f9403aa7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = [ x+' '+y +' ' + z+' ' + a+' ' + b  for x,y,z,a,b in zip(eastern_sentences[0::5], eastern_sentences[1::5], eastern_sentences[2::5], eastern_sentences[3::5], eastern_sentences[4::5]) ]\n",
    "# p2 = [ x+' '+y +' ' + z+' ' + a+' ' + b for x,y,z,a,b in zip(southwestern_sentences[0::5], southwestern_sentences[1::5], southwestern_sentences[2::5], southwestern_sentences[3::5], southwestern_sentences[4::5]) ]\n",
    "# X5 = p1 + p2\n",
    "\n",
    "# Y5 = np.concatenate((np.zeros(len(p1)),np.ones(len(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69a348f1-82ff-42db-ab38-99b44338c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "# skf.get_n_splits(X5, Y5)\n",
    "# all_weights = np.zeros((118,))\n",
    "# clf2 = make_pipeline(StandardScaler(),\n",
    "#                     SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')) #loss huber\n",
    "\n",
    "\n",
    "    \n",
    "# featurized_x =  np.zeros((len(initialized_nGram_dict_sw_e_100),len(X5)))\n",
    "# # featurized_x = compute_n_grams(X,ngrams_count)\n",
    "# for index,sentence in enumerate(featurized_x):\n",
    "#     features = compute_n_grams(X5[index],ngrams_count)\n",
    "#     new_dict = copy.deepcopy(initialized_nGram_dict_sw_e_100)\n",
    "#     for something in features.keys():\n",
    "#             if something in new_dict.keys():\n",
    "#                 new_dict[something] = features[something]\n",
    "#     featurized_x[:,index] = np.fromiter(new_dict.values(), dtype=float)\n",
    "    \n",
    "# single_sentence_inputs = []\n",
    "# single_sentence_labels = []\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X5, Y5)):\n",
    "#     clf2.fit(featurized_x[:,train_index].T, Y5[train_index])\n",
    "    \n",
    "#     single_sentence_inputs.extend(clf2.predict(featurized_x[:,test_index].T))\n",
    "#     single_sentence_labels.extend(Y5[test_index])\n",
    "#     all_weights += np.asarray(clf2[1].coef_[0])\n",
    "    \n",
    "# results = np.asarray(single_sentence_inputs).reshape(-1)\n",
    "# truth_labels = np.asarray(single_sentence_labels).reshape(-1)\n",
    "\n",
    "# print(classification_report(truth_labels,results,target_names=['e','sw']))\n",
    "# print(confusion_matrix(truth_labels,results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b1d3a81-4e84-4481-9eb1-416e91185cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dictionaries can be searched\n",
    "# all_sw_ngrams['wi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a4ae87b-08e9-4d12-8a39-18052428e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_e_ngrams['wi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4d63c-9d33-43dd-b976-dede32aa79b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
